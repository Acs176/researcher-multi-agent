{"eval_id": "eval_001", "run_id": "57535247-f131-4a07-8b9c-6e2a8a9b9ba8", "started_at": "2026-02-02T16:26:11.370739+00:00", "query": "Explain the difference between precision and recall, with a simple example.", "answer": "Precision and recall are two ways to measure a classifier\u2019s performance when outcomes are binary (positive vs. negative). They focus on different kinds of error.\n\nDefinitions (using standard terms)\n- True positives (TP): items correctly predicted positive.\n- False positives (FP): items predicted positive but actually negative.\n- False negatives (FN): items predicted negative but actually positive.\n\n- Precision = TP / (TP + FP).  \n  (Of all items predicted positive, how many are actually positive?)\n- Recall = TP / (TP + FN).  \n  (Of all actual positives, how many did the model find?)\n\nSimple example (spam classifier)\n- There are 100 emails, 20 are actually spam.\n- The classifier labels 25 emails as \u201cspam\u201d.\n- Of those 25 predicted spam, 15 are truly spam \u2192 TP = 15, FP = 10.\n- There are 5 actual spam that the classifier missed \u2192 FN = 5.\n\nCompute\n- Precision = 15 / (15 + 10) = 15 / 25 = 0.60 (60% of predicted spam are real spam).\n- Recall = 15 / (15 + 5) = 15 / 20 = 0.75 (the classifier found 75% of all actual spam).\n\nInterpretation and trade-off\n- High precision means few false positives (when it says \u201cpositive,\u201d it\u2019s usually right).\n- High recall means few false negatives (it finds most of the actual positives).\n- Often improving one reduces the other; choose based on cost of errors (e.g., in disease screening you may prefer high recall to avoid missing cases; in email filtering you might prefer high precision to avoid flagging good mail).\n\nOptional summary metric\n- F1 score = 2 * (precision * recall) / (precision + recall) gives a single number balancing both.", "browsed": false, "browse_calls": 0, "sources": [], "latency_ms": 12432.92336200102, "token_counts": {"prompt": null, "completion": null, "total": null}, "model_id": "gpt-5-mini"}
{"eval_id": "eval_001", "run_id": "960d4db6-5b99-4ee9-b66b-4c6653d11ecb", "started_at": "2026-02-02T16:26:23.804863+00:00", "query": "Identify 3 reputable benchmarks for evaluating LLM factuality and briefly describe what each measures. Cite original papers/sites.", "answer": "Here are three reputable, widely\u2011used benchmarks for evaluating LLM factuality, with what each measures and original-paper/site links.\n\n1) FEVER (Fact Extraction and VERification) \u2014 Thorne et al., ACL 2018  \n- What it measures: claim verification \u2014 given a textual claim, the benchmark tests whether a model can (a) find supporting evidence from Wikipedia and (b) classify the claim as Supported / Refuted / NotEnoughInfo. Used to evaluate fact\u2011checking/evidence retrieval + reasoning.  \n- Key metrics: label accuracy and evidence retrieval precision/recall/F1.  \n- Paper / dataset: https://aclanthology.org/N18-1074/ and dataset page https://fever.ai/dataset/fever.html\n\n2) TruthfulQA \u2014 Lin et al. (arXiv 2021)  \n- What it measures: propensity of a model to produce false or misleading answers (hallucinations) on open-ended question prompts across domains (health, law, finance, etc.). Designed to reveal when models \"mimic human falsehoods\" rather than produce truthful answers.  \n- Key metrics: human\u2011judged truthfulness / accuracy against gold answers and adversarial splits; scores emphasize avoidance of confidently stated falsehoods.  \n- Paper / benchmark: https://arxiv.org/abs/2109.07958\n\n3) QAGS (Asking and Answering Questions to Evaluate Factual Consistency of Summaries) \u2014 Wang et al., ACL 2020  \n- What it measures: factual consistency (faithfulness) of generated summaries relative to source documents. QAGS generates questions from a candidate summary, answers them using (a) the summary and (b) the source document, and compares answers to detect summary hallucinations/inconsistencies.  \n- Key metrics: automated QA\u2011based consistency score (answer overlap / similarity, e.g., F1 or learned answer similarity); correlates with human judgments of factuality.  \n- Paper: https://aclanthology.org/2020.acl-main.450/ (arXiv: https://arxiv.org/abs/2004.04228)\n\nIf you want, I can: (a) add one more benchmark (e.g., LAMA for probing memorized factual knowledge), (b) show typical evaluation scripts/metrics used for each, or (c) point to code repos and leaderboard pages.", "browsed": true, "browse_calls": 5, "sources": [{"url": "https://aclanthology.org/N18-1074/", "title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification - ACL Anthology", "retrieved_at": "2026-02-02T16:26:34.252038+00:00", "snippets": ["James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018."], "query": "FEVER dataset paper fact verification 2018 dataset original paper URL TruthfulQA paper 2021 QAGS paper factuality summarization 2020 FactCC 2020 paper"}, {"url": "https://fever.ai/dataset/fever.html", "title": "Fact Extraction and VERification", "retrieved_at": "2026-02-02T16:26:34.252038+00:00", "snippets": ["FEVER (Fact Extraction and VERification) consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo."], "query": "FEVER dataset paper fact verification 2018 dataset original paper URL TruthfulQA paper 2021 QAGS paper factuality summarization 2020 FactCC 2020 paper"}, {"url": "https://www.semanticscholar.org/paper/TruthfulQA:-Measuring-How-Models-Mimic-Human-Lin-Hilton/77d956cdab4508d569ae5741549b78e715fd0749", "title": "[PDF] TruthfulQA: Measuring How Models Mimic Human Falsehoods | Semantic Scholar", "retrieved_at": "2026-02-02T16:26:43.510275+00:00", "snippets": ["QAGS (pronounced \u201ckags\u201d), an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary, is proposed and is believed to be a promising tool in automatically generating usable and factually consistent text.Expand ... Joshua MaynezShashi NarayanBernd BohnetRyan T. McDonald ... It is found that neural abstractive summarization models are highly prone to hallucinate content that is unfaithful to the input document and textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.Expand"], "query": "TruthfulQA paper 2021 dataset benchmark 'TruthfulQA: Measuring How Models Mimic Human Falsehoods' URL QAGS 'Evaluating the Factual Consistency of Abstractive Summarization' 2020 Kryscinski FactCC 2020 'FactCC: A Dependency Parsing Approach'? 'FactCC' paper URL"}, {"url": "https://arxiv.org/abs/2109.07958", "title": "[2109.07958] TruthfulQA: Measuring How Models Mimic Human Falsehoods", "retrieved_at": "2026-02-02T16:26:43.510275+00:00", "snippets": ["We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics."], "query": "TruthfulQA paper 2021 dataset benchmark 'TruthfulQA: Measuring How Models Mimic Human Falsehoods' URL QAGS 'Evaluating the Factual Consistency of Abstractive Summarization' 2020 Kryscinski FactCC 2020 'FactCC: A Dependency Parsing Approach'? 'FactCC' paper URL"}, {"url": "https://arxiv.org/html/2210.02804", "title": "Just ClozE! A Novel Framework for Evaluating the Factual Consistency Faster in Abstractive Summarization", "retrieved_at": "2026-02-02T16:26:53.680811+00:00", "snippets": ["Maynez et al. Maynez, J., Narayan, S., Bohnet, B., and McDonald, R. (2020). On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp."], "query": "QAGS Evaluating the Factual Consistency of Abstractive Summarization Maynez 2020 arXiv QAGS paper URL 'QAGS' 'Maynez' '2020'"}, {"url": "https://ui.adsabs.harvard.edu/abs/2020arXiv200404228W/abstract", "title": "Asking and Answering Questions to Evaluate the Factual Consistency of Summaries - ADS", "retrieved_at": "2026-02-02T16:26:53.680811+00:00", "snippets": ["Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose an automatic evaluation protocol called QAGS (pronounced \"kags\") that is designed to identify factual inconsistencies in a generated summary."], "query": "QAGS Evaluating the Factual Consistency of Abstractive Summarization Maynez 2020 arXiv QAGS paper URL 'QAGS' 'Maynez' '2020'"}, {"url": "https://aclanthology.org/2020.acl-main.450/", "title": "Asking and Answering Questions to Evaluate the Factual Consistency of Summaries - ACL Anthology", "retrieved_at": "2026-02-02T16:27:02.117592+00:00", "snippets": ["%0 Conference Proceedings %T Asking and Answering Questions to Evaluate the Factual Consistency of Summaries %A Wang, Alex %A Cho, Kyunghyun %A Lewis, Mike %Y Jurafsky, Dan %Y Chai, Joyce %Y Schluter, Natalie %Y Tetreault, Joel %S Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics %D 2020 %8 July %I Association for Computational Linguistics %C Online %F wang-etal-2020-asking %X Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose QAGS (pronounced \u201ckags\u201d), an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary."], "query": "QAGS 'Asking and Answering Questions to Evaluate the Factual Consistency of Summaries' 2020 Wang Narayan arXiv 'QAGS' 'Wang' '2020' 'Asking and Answering'"}, {"url": "https://www.semanticscholar.org/paper/Asking-and-Answering-Questions-to-Evaluate-the-of-Wang-Cho/d36e39aedd802aea4be1ea303c70dc56e97dbc3c", "title": "[PDF] Asking and Answering Questions to Evaluate the Factual Consistency of Summaries | Semantic Scholar", "retrieved_at": "2026-02-02T16:27:02.117592+00:00", "snippets": ["QAGS (pronounced \u201ckags\u201d), an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary, is proposed and is believed to be a promising tool in automatically generating usable and factually ..."], "query": "QAGS 'Asking and Answering Questions to Evaluate the Factual Consistency of Summaries' 2020 Wang Narayan arXiv 'QAGS' 'Wang' '2020' 'Asking and Answering'"}, {"url": "https://github.com/W4ngatang/qags", "title": "GitHub - W4ngatang/qags: Question Answering and Generation for Summarization", "retrieved_at": "2026-02-02T16:27:17.674430+00:00", "snippets": ["Make sure model_dir points to the QA model directory. To compute QAGS scores, evaluate the QA model using the both the article as context and the summary as context, so you will need to run this command twice."], "query": "QAGS method compare answers how measured 'QAGS' 'Asking and Answering Questions to Evaluate the Factual Consistency of Summaries' answer comparison metric 'BERTScore' 'F1'"}, {"url": "https://arxiv.org/abs/2004.04228", "title": "[2004.04228] Asking and Answering Questions to Evaluate the Factual Consistency of Summaries", "retrieved_at": "2026-02-02T16:27:17.674430+00:00", "snippets": ["To evaluate QAGS, we collect human judgments of factual consistency on model-generated summaries for the CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) summarization datasets."], "query": "QAGS method compare answers how measured 'QAGS' 'Asking and Answering Questions to Evaluate the Factual Consistency of Summaries' answer comparison metric 'BERTScore' 'F1'"}], "latency_ms": 80243.01349400048, "token_counts": {"prompt": null, "completion": null, "total": null}, "model_id": "gpt-5-mini"}
{"eval_id": "eval_001", "run_id": "23df08e7-23d4-4974-9dc1-448fcb6c1fcf", "started_at": "2026-02-03T07:48:41.772366+00:00", "query": "Identify 3 reputable benchmarks for evaluating LLM factuality and briefly describe what each measures. Cite original papers/sites.", "answer": "[{\"url\": \"https://arxiv.org/abs/2109.07958\", \"status\": 200, \"text\": \"[2109.07958] TruthfulQA: Measuring How Models Mimic Human Falsehoods Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate > cs > arXiv:2109.07958 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computation and Language arXiv:2109.07958 (cs) [Submitted on 8 Sep 2021 ( v1 ), last revised 8 May 2022 (this version, v2)] Title: TruthfulQA: Measuring How Models Mimic Human Falsehoods Authors: Stephanie Lin , Jacob Hilton , Owain Evans View a PDF of the paper titled TruthfulQA: Measuring How Models Mimic Human Falsehoods, by Stephanie Lin and 2 other authors View PDF Abstract: We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web. Comments: ACL 2022 (main conferenc...\", \"robots_allowed\": true, \"error\": null}, {\"url\": \"https://arxiv.org/abs/1803.05355\", \"status\": 200, \"text\": \"[1803.05355] FEVER: a large-scale dataset for Fact Extraction and VERification Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate > cs > arXiv:1803.05355 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computation and Language arXiv:1803.05355 (cs) [Submitted on 14 Mar 2018 ( v1 ), last revised 18 Dec 2018 (this version, v3)] Title: FEVER: a large-scale dataset for Fact Extraction and VERification Authors: James Thorne , Andreas Vlachos , Christos Christodoulopoulos , Arpit Mittal View a PDF of the paper titled FEVER: a large-scale dataset for Fact Extraction and VERification, by James Thorne and 2 other authors View PDF Abstract: In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss $\\\\kappa$. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources. Comments: Updated version of NAACL2018 paper. Data is released on...\", \"robots_allowed\": true, \"error\": null}, {\"url\": \"https://arxiv.org/abs/1910.12840\", \"status\": 200, \"text\": \"[1910.12840] Evaluating the Factual Consistency of Abstractive Text Summarization Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate > cs > arXiv:1910.12840 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computation and Language arXiv:1910.12840 (cs) [Submitted on 28 Oct 2019] Title: Evaluating the Factual Consistency of Abstractive Text Summarization Authors: Wojciech Kry\\u015bci\\u0144ski , Bryan McCann , Caiming Xiong , Richard Socher View a PDF of the paper titled Evaluating the Factual Consistency of Abstractive Text Summarization, by Wojciech Kry\\\\'sci\\\\'nski and 3 other authors View PDF Abstract: Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that th...\", \"robots_allowed\": true, \"error\": null}]", "browsed": true, "browse_calls": 3, "sources": [{"url": "https://arxiv.org/abs/2109.07958", "title": "[2109.07958] TruthfulQA: Measuring How Models Mimic Human Falsehoods", "retrieved_at": "2026-02-03T07:49:05.338297+00:00", "snippets": ["View a PDF of the paper titled TruthfulQA: Measuring How Models Mimic Human Falsehoods, by Stephanie Lin and 2 other authors View PDF"], "query": "TruthfulQA 'TruthfulQA: Measuring How Models Mimic Human Falsehoods' paper authors and link"}, {"url": "https://aclanthology.org/2022.acl-long.229/", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods - ACL Anthology", "retrieved_at": "2026-02-03T07:49:05.338297+00:00", "snippets": ["... @inproceedings{lin-etal-2022-truthfulqa, title = \"{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods\", author = \"Lin, Stephanie and Hilton, Jacob and Evans, Owain\", editor = \"Muresan, Smaranda and Nakov, Preslav and Villavicencio, ..."], "query": "TruthfulQA 'TruthfulQA: Measuring How Models Mimic Human Falsehoods' paper authors and link"}, {"url": "https://arxiv.org/abs/1803.05355", "title": "[1803.05355] FEVER: a large-scale dataset for Fact Extraction and VERification", "retrieved_at": "2026-02-03T07:49:06.474866+00:00", "snippets": ["We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors. Donate ... View a PDF of the paper titled FEVER: a large-scale dataset for Fact Extraction and VERification, by James Thorne and 2 other authors View PDF"], "query": "FEVER 'FEVER: a large-scale dataset for Fact Extraction and VERification' Thorne et al. 2018 paper link"}, {"url": "https://aclanthology.org/N18-1074/", "title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification - ACL Anthology", "retrieved_at": "2026-02-03T07:49:06.474866+00:00", "snippets": ["%0 Conference Proceedings %T FEVER: ... It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from..."], "query": "FEVER 'FEVER: a large-scale dataset for Fact Extraction and VERification' Thorne et al. 2018 paper link"}, {"url": "https://arxiv.org/abs/1910.12840", "title": "[1910.12840] Evaluating the Factual Consistency of Abstractive Text Summarization", "retrieved_at": "2026-02-03T07:49:07.559339+00:00", "snippets": ["The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract ..."], "query": "FactCC QAGS 'FactCC factual consistency summarization' 'QAGS evaluating factual consistency of abstractive summaries' papers and links"}, {"url": "https://github.com/salesforce/factCC", "title": "GitHub - salesforce/factCC: Resources for the \"Evaluating the Factual Consistency of Abstractive Text Summarization\" paper", "retrieved_at": "2026-02-03T07:49:07.559339+00:00", "snippets": ["Resources for the \"Evaluating the Factual Consistency of Abstractive Text Summarization\" paper - salesforce/factCC"], "query": "FactCC QAGS 'FactCC factual consistency summarization' 'QAGS evaluating factual consistency of abstractive summaries' papers and links"}], "latency_ms": 47123.553632999574, "token_counts": {"prompt": null, "completion": null, "total": null}, "model_id": "gpt-5-mini"}
{"eval_id": "eval_001", "run_id": "6fce0882-0f37-40df-9cef-541affb13a7f", "started_at": "2026-02-03T07:50:46.545206+00:00", "query": "Identify 3 reputable benchmarks for evaluating LLM factuality and briefly describe what each measures. Cite original papers/sites.", "answer": "[{\"url\": \"https://arxiv.org/abs/1803.05355\", \"status\": 200, \"text\": \"[1803.05355] FEVER: a large-scale dataset for Fact Extraction and VERification Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate > cs > arXiv:1803.05355 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computation and Language arXiv:1803.05355 (cs) [Submitted on 14 Mar 2018 ( v1 ), last revised 18 Dec 2018 (this version, v3)] Title: FEVER: a large-scale dataset for Fact Extraction and VERification Authors: James Thorne , Andreas Vlachos , Christos Christodoulopoulos , Arpit Mittal View a PDF of the paper titled FEVER: a large-scale dataset for Fact Extraction and VERification, by James Thorne and 2 other authors View PDF Abstract: In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss $\\\\kappa$. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources. Comments: Updated version of NAACL2018 paper. Data is released on...\", \"robots_allowed\": true, \"error\": null}, {\"url\": \"https://arxiv.org/abs/2109.07958\", \"status\": 200, \"text\": \"[2109.07958] TruthfulQA: Measuring How Models Mimic Human Falsehoods Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate > cs > arXiv:2109.07958 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computation and Language arXiv:2109.07958 (cs) [Submitted on 8 Sep 2021 ( v1 ), last revised 8 May 2022 (this version, v2)] Title: TruthfulQA: Measuring How Models Mimic Human Falsehoods Authors: Stephanie Lin , Jacob Hilton , Owain Evans View a PDF of the paper titled TruthfulQA: Measuring How Models Mimic Human Falsehoods, by Stephanie Lin and 2 other authors View PDF Abstract: We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web. Comments: ACL 2022 (main conferenc...\", \"robots_allowed\": true, \"error\": null}, {\"url\": \"https://arxiv.org/abs/1910.12840\", \"status\": 200, \"text\": \"[1910.12840] Evaluating the Factual Consistency of Abstractive Text Summarization Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate > cs > arXiv:1910.12840 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computation and Language arXiv:1910.12840 (cs) [Submitted on 28 Oct 2019] Title: Evaluating the Factual Consistency of Abstractive Text Summarization Authors: Wojciech Kry\\u015bci\\u0144ski , Bryan McCann , Caiming Xiong , Richard Socher View a PDF of the paper titled Evaluating the Factual Consistency of Abstractive Text Summarization, by Wojciech Kry\\\\'sci\\\\'nski and 3 other authors View PDF Abstract: Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that th...\", \"robots_allowed\": true, \"error\": null}]", "browsed": true, "browse_calls": 3, "sources": [{"url": "https://arxiv.org/abs/1803.05355", "title": "[1803.05355] FEVER: a large-scale dataset for Fact Extraction and VERification", "retrieved_at": "2026-02-03T07:51:08.942827+00:00", "snippets": ["View a PDF of the paper titled FEVER: a large-scale dataset for Fact Extraction and VERification, by James Thorne and 2 other authors View PDF"], "query": "FEVER: 'FEVER: a Large-scale Dataset for Fact Extraction and VERification' Thorne 2018 paper PDF"}, {"url": "https://aclanthology.org/N18-1074/", "title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification - ACL Anthology", "retrieved_at": "2026-02-03T07:51:08.942827+00:00", "snippets": ["Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) ... James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a Large-scale Dataset for Fact Extraction and VERification."], "query": "FEVER: 'FEVER: a Large-scale Dataset for Fact Extraction and VERification' Thorne 2018 paper PDF"}, {"url": "https://arxiv.org/abs/2109.07958", "title": "[2109.07958] TruthfulQA: Measuring How Models Mimic Human Falsehoods", "retrieved_at": "2026-02-03T07:51:10.172153+00:00", "snippets": ["The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans."], "query": "TruthfulQA: 'TruthfulQA: Measuring How Models Mimic Human Falsehoods' paper"}, {"url": "https://aclanthology.org/2022.acl-long.229/", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods - ACL Anthology", "retrieved_at": "2026-02-03T07:51:10.172153+00:00", "snippets": ["%0 Conference Proceedings %T ... Ireland %F lin-etal-2022-truthfulqa %X We propose a benchmark to measure whether a language model is truthful in generating answers to questions...."], "query": "TruthfulQA: 'TruthfulQA: Measuring How Models Mimic Human Falsehoods' paper"}, {"url": "https://arxiv.org/abs/1910.12840", "title": "[1910.12840] Evaluating the Factual Consistency of Abstractive Text Summarization", "retrieved_at": "2026-02-03T07:51:11.195706+00:00", "snippets": ["View a PDF of the paper titled Evaluating the Factual Consistency of Abstractive Text Summarization, by Wojciech Kry\\'sci\\'nski and 3 other authors View PDF"], "query": "QAGS: 'QAGS: Evaluating the Factual Consistency of Abstractive Summaries' Kryscinski paper PDF"}, {"url": "https://www.researchgate.net/publication/347236463_Evaluating_the_Factual_Consistency_of_Abstractive_Text_Summarization", "title": "Evaluating the Factual Consistency of Abstractive Text Summarization | Request PDF", "retrieved_at": "2026-02-03T07:51:11.195706+00:00", "snippets": ["Request PDF | On Jan 1, 2020, Wojciech Kryscinski and others published Evaluating the Factual Consistency of Abstractive Text Summarization | Find, read and cite all the research you need on ResearchGate"], "query": "QAGS: 'QAGS: Evaluating the Factual Consistency of Abstractive Summaries' Kryscinski paper PDF"}], "latency_ms": 37065.794940999695, "token_counts": {"prompt": null, "completion": null, "total": null}, "model_id": "gpt-5-mini"}
{"eval_id": "eval_001", "run_id": "b5f244ea-f140-4569-8150-def7d3219072", "started_at": "2026-02-03T08:10:50.095978+00:00", "query": "Identify 3 reputable benchmarks for evaluating LLM factuality and briefly describe what each measures. Cite original papers/sites.", "answer": "[{\"url\": \"https://arxiv.org/abs/2109.07958\", \"status\": 200, \"text\": \"[2109.07958] TruthfulQA: Measuring How Models Mimic Human Falsehoods Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate > cs > arXiv:2109.07958 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computation and Language arXiv:2109.07958 (cs) [Submitted on 8 Sep 2021 ( v1 ), last revised 8 May 2022 (this version, v2)] Title: TruthfulQA: Measuring How Models Mimic Human Falsehoods Authors: Stephanie Lin , Jacob Hilton , Owain Evans View a PDF of the paper titled TruthfulQA: Measuring How Models Mimic Human Falsehoods, by Stephanie Lin and 2 other authors View PDF Abstract: We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web. Comments: ACL 2022 (main conferenc...\", \"robots_allowed\": true, \"error\": null}, {\"url\": \"https://aclanthology.org/N18-1074/\", \"status\": 200, \"text\": \"FEVER: a Large-scale Dataset for Fact Extraction and VERification - ACL Anthology ACL Anthology News (current) FAQ (current) Corrections (current) Submissions (current) GitHub FEVER : a Large-scale Dataset for Fact Extraction and VER ification James Thorne , Andreas Vlachos , Christos Christodoulopoulos , Arpit Mittal Correct Metadata for \\u00d7 Use this form to create a GitHub issue with structured data describing the correction. You will need a GitHub account.\\nOnce you create that issue, the correction will be reviewed by a staff member. \\u26a0\\ufe0f Mobile Users: Submitting this form to create a new issue will only work with github.com, not the GitHub Mobile app. Important : The Anthology treat PDFs as authoritative. Please use this form only to correct data that is out of line with the PDF. See our corrections guidelines if you need to change the PDF. Title Adjust the title. Retain tags such as <fixed-case>. Authors Adjust author names and order to match the PDF. Add Author Abstract Correct abstract if needed. Retain XML formatting tags such as <tex-math>. You may use <b>...</b> for bold , <i>...</i> for italic , and <url>...</url> for URLs. Verification against PDF Ensure that the new title/authors match the snapshot below. (If there is no snapshot or it is too small, consult the PDF .) Authors concatenated from the text boxes above: ALL author names match the snapshot above\\u2014including middle initials, hyphens, and accents. Create GitHub issue for\\u00a0staff review Abstract In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necess...\", \"robots_allowed\": true, \"error\": null}, {\"url\": \"https://aclanthology.org/2022.naacl-main.187.pdf\", \"status\": 200, \"text\": \"%PDF-1.5\\n%\\ufffd\\ufffd\\ufffd\\ufffd\\n247 0 obj < < /Length 353       \\n/Filter /FlateDecode\\n>>\\nstream\\nx\\u069dR\\ufffdn\\ufffd0\\u0010\\ufffd\\ufffd\\u0015>\\ufffd\\u0014\\ufffd\\u0019\\u001b\\ufffd\\ufffd-\\ufffd\\u001eEU+qk{@\\ufffd,R\\ufffd)\\u02e1\\u007f_\\ufffdI\\u001aE\\ufffd*\\u0015$\\ufffd1\\u02db\\ufffd'\\ufffd\\u0013gy\\ufffd9\\ufffd4\\u000e\\u00c0$9\\ufffd\\ufffd\\u0006L\\u0012\\u001eR)H\\ufffd#\\ufffd\\ufffds\\ufffd\\u0005\\ufffd\\u0559R\\ufffd\\ufffd):\\ufffdg\\u0002\\\\\\ufffd\\ufffd\\u0617\\ufffd\\u0002\\u0006\\ufffdYt\\ufffd\\ufffd\\\\\\ufffd\\u001e\\ufffd\\\\\\ufffdd\\ufffd\\ufffd'\\ufffd\\ufffd\\ufffd\\ufffd\\ufffdZ\\ufffdU\\ufffd6\\ufffdt\\ufffd\\u001ez3\\ufffd\\ufffd\\u062a\\ufffdtV\\ufffd}\\ufffd\\ufffd\\ufffd\\\\\\ufffd\\u01ed\\ufffda\\ufffdJ\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd;T]_e\\u0755\\ufffd\\ufffdl\\b2\\ufffd\\u001c\\u0004\\ufffd\\u0011h,b{\\ufffd\\ufffd\\ufffd>\\ufffd\\u07a6f \\ufffd\\u0010\\ufffdb\\ufffd\\ufffd;\\u0013/f\\ufffd\\ufffd < \\u0014n\\ufffd\\u8f5e\\ufffd\\ufffdFJc^pf\\u001e\\\"\\ufffd\\ufffdd\\ufffdR/,\\ufffd!-\\ufffdl\\u001b\\u00132\\ufffd\\u021f\\u0013!\\ufffd\\ufffd\\u01c0\\ufffd0\\ufffd\\u0010\\ufffd\\u0019\\ufffd\\ufffd\\ufffd\\u0011\\ufffdG\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\t\\ufffd\\ufffd\\ufffd\\ufffd;\\ufffd\\u0000\\u0007Hat\\ufffd;\\ufffd\\ufffd\\ufffd\\u0006\\u0000A\\ufffd\\u0003\\u0005\\ufffd6H\\ufffd\\ufffd\\u0387\\ufffd\\u0014\\ufffd#l\\ufffd\\u0019\\ufffd\\ufffdynN,\\u001f\\ufffd\\ufffdh\\ufffd\\u017cG\\ufffd\\ufffd\\ufffdb\\ufffd|Z\\ufffd\\ufffd\\ufffdi@Fo \\ufffd\\ufffdk&#*CN\\u0004P\\u0011Ik\\ufffdd\\ufffd\\ufffda\\ufffd\\u0000\\ufffd\\ufffd\\ufffdt\\nendstream\\nendobj\\n75 0 obj < < /Type /XObject\\n/Subtype /Form\\n/FormType 1\\n/PTEX.FileName (./papers/494.pdf)\\n/PTEX.PageNumber 1\\n/PTEX.InfoDict 254 0 R\\n/BBox [0 0 595.276 841.89]\\n/Resources < < /ColorSpace < < /pgfprgb [/Pattern/DeviceRGB]\\n>>/ExtGState < < >>/Font < < /F127 257 0 R /F133 260 0 R /F169 263 0 R /F85 266 0 R /F86 269 0 R /F88 272 0 R >>\\n/Pattern < < >>/ProcSet [ /PDF /Text ]\\n>>\\n/Length 4615\\n/Filter /FlateDecode\\n>>\\nstream\\nx\\ufffd\\ufffd;\\ufffdr\\ufffd\\ufffdr\\ufffd\\ufffd\\u0015|I\\u0005\\ufffd2\\u0011\\ufffd\\u0015\\ufffd\\ufffd}\\u058e\\ufffduR\\ufffdU\\u0169\\ufffd\\ufffdaDB\\\"\\ufffd !\\ufffdb\\ufffd:u\\ufffd\\ufffdtO\\ufffd\\ufffd\\u0000\\u0004JKi+\\ufffd*q0\\ufffdKwO_\\ufffd\\u001b\\ufffd\\ufffdv\\ufffd\\ufffd\\ufffd\\u007f\\ufffd\\ufffd~\\ufffd\\ufffdz\\ufffdO\\ufffd\\t\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffdruu\\ufffd\\u0012\\ufffd\\ufffdSeV\\ufffd\\ufffd\\ufffdP\\ufffd\\ufffdj\\ufffd\\ufffd5\\ufffd\\ufffd\\ufffdwki\\\"\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\u0016I\\u4aaf\\ufffd\\u001b\\u0005\\ufffd\\u001f\\u000ew\\ufffdZdQ\\ufffd\\ufffd\\ufffdU\\ufffd[o\\ufffdI`\\ufffd\\ufffd\\u001b\\ufffd\\ufffd\\ufffdoW\\ufffd\\ufffd\\u0698$\\ufffdi\\ufffd\\ufffd\\b\\u0013\\ufffd\\ufffd\\f+\\ufffd\\ufffd\\ufffd\\u0019\\ufffd\\ufffd\\u01f6l\\ufffd\\ufffd\\ufffdD\\u001da\\ufffd\\ufffdue}\\ufffd\\ufffd\\u001b\\ufffdU7\\ufffd\\ufffd?\\u001c\\\\S\\ufffdI\\ufffda#\\ufffdF\\u01391\\u001e\\u001b\\u000bM\\ufffd\\ufffd\\u0000\\ufffd8\\ufffd\\ufffd6~]\\u0015\\u001f\\ufffdqW\\ufffd\\\"\\ufffdb\\ufffd%x\\ufffdq\\ufffd\\ufffd\\\\o\\ufffd\\ufffdi_\\u0016\\ufffd\\ufffd\\ufffd}q\\ufffd\\ufffdQ\\ufffd\\ufffd\\ufffd_\\uf452:\\ufffd\\ufffd\\u0000N\\np\\ufffdh\\ufffd_\\ufffd\\ufffdD\\ufffdq\\ufffdj\\u001a\\ufffd\\ufffd\\ufffd\\ufffdJ\\ufffd < \\ufffda\\ufffd\\ufffd\\u0006xo\\u0019`\\ufffd\\ufffd\\u0000\\u0003\\ufffd\\ufffd\\ufffd\\ufffdhJ\\ufffd\\ufffdwU\\ufffd\\ufffd\\ufffd\\u0376\\ufffd\\u066f\\u007f`\\u020b\\ufffdp\\ufffdvO$\\u03928\\ufffdA\\ufffd\\u007f\\ufffd\\ufffdS\\ufffd\\u001a\\u0010\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd>\\ufffdG\\u000fk\\\\\\ufffd=wl?\\\"h\\u007f\\ufffd\\ufffdv\\ufffd3\\ufffd\\ufffd\\u0007\\ufffd\\ufffd\\u0001\\ufffd < \\ufffdF\\ufffdF\\u0002\\bZ\\ufffd2g\\u0010^_\\ufffd]\\u0003\\ufffd|n\\u0005\\ufffd>\\ufffds\\ufffdg\\u0003\\ufffd\\u01c9\\ufffd\\u0017\\ufffdJ\\ufffd\\ufffd4\\ufffdR\\ufffd29\\ufffd\\ufffd`\\ufffd\\ufffd\\ufffd\\u0018q\\ufffd\\u0018%t\\ufffd\\u001d8f\\rO\\ufffd\\ufffd\\ufffdl\\ufffd\\ufffd\\u001d\\ufffdh\\ufffd\\ufffd\\u0615a\\ufffd\\ufffd0\\ufffd\\ufffd\\ufffdX`\\ufffd\\u001bO\\ufffd$\\u0006p\\ufffd\\ufffdf26\\ufffd\\ufffd~\\u07ee\\ufffd\\ufffd?v@!-\\ufffdv\\ufffdt\\ufffd\\ufffdP\\uf2aa\\ufffdv\\ufffd}w\\ufffdu\\ufffd\\ufffd\\u030d\\ufffd\\ufffd\\ufffd\\ufffd\\ufffdo\\ufffdaC=\\ufffd1\\u0005\\u0011\\ufffd\\ufffd\\u96cf\\ufffd\\u0016s\\ufffd\\ufffd\\ufffd\\ufffd\\u0010\\ufffd\\ufffd\\u0003=\\ufffd\\r\\ufffd\\ufffd\\n\\ufffd\\ufffd? < \\ufffdz\\t\\ufffd\\ufffd*\\ufffd\\ufffdH\\u0001\\u001c\\ufffd+\\u000f\\u0005\\u0010\\ufffdD\\u0007\\ufffd\\u5f0c,Q\\ufffdd\\ufffd\\ufffd\\ufffd\\ufffd5p\\ufffdN\\u0014\\bA\\ufffdv\\ufffd'|\\ufffd\\ufffdD\\ufffd\\ufffd\\u001a0G\\u0011\\ufffd\\u0011\\u5c6b\\ufffd\\ufffd\\u076f\\ufffd\\ufffdUy,Zj\\u0002y\\ufffdoSl\\ufffd\\u0389\\ufffdm\\ufffdm\\ufffd\\nF\\ufffd\\ufffd\\u00199Qg E\\ufffd++\\ufffd\\ufffd\\ufffd\\\\{E\\u0002\\ufffd)\\ufffd\\ufffd^\\ufffd\\ufffd\\u0017m8\\u0001\\ufffdn\\ufffdA\\ufffde\\u0012m\\ufffd\\ufffd4\\ufffd\\u0007\\ufffd\\u8265T\\u001e\\ufffd51\\ufffd\\u03ef\\ufffd\\b\\ufffd(\\ufffdD\\ufffd\\ufffdk\\ufffdm\\u000b\\ufffd(\\ufffdhK\\u001c\\ufffd+A\\ufffdhPA\\\"*\\u001a\\ufffd\\ufffd\\u0006\\u0013s\\ufffd=\\ufffd\\ufffd\\ufffdq\\ufffd\\b\\ufffd\\u001d\\ufffd\\ufffd\\ufffd\\ufffd-2F\\u007f\\ufffd,\\ufffd\\ufffd\\u001d\\ufffd\\ufffd\\ufffd\\ufffd*\\u070eZHc\\ufffd\\u0005f\\u0007Ib\\ufffd\\ufffd\\ufffd\\ufffdqv;\\ufffd\\u0119\\ufffd\\ufffd\\u0776\\ufffd\\tp\\u000fPN\\ufffd\\u007f\\ufffd\\ufffd\\ufffd{\\ufffd/A\\ufffd\\ufffd\\ufffdk\\u072e\\ufffd=\\ufffdS\\u0440\\ufffd\\u001f\\ufffd\\ufffd}qf/\\ufffdgq\\ufffd\\ufffd\\ufffd GI>\\u0007AfA\\ufffd\\u0007l\\ufffd\\u0003\\ufffdQ\\ufffd\\ufffd\\u0019\\ufffd\\ufffdA\\ufffd\\ufffd\\u0011;Q?3R\\ufffd\\ufffdj\\ufffd\\f\\ufffd~\\u0013D4Og\\ufffd\\ufffd\\u001a\\ufffd1e\\u0006A\\ufffd\\ufffd+\\u000e@^8\\u001c\\u0016BX\\ufffd\\u06fb\\ufffdZ[\\ufffd\\u00147}U\\ufffdM\\ufffd\\ufffd\\ufffd\\u001d\\r\\u001bp\\u00123\\r\\ufffd\\ufffd\\u0002DGH\\ufffdG\\ufffd\\ufffd%v'\\ufffdl\\ufffd\\ufffdq\\ufffdB\\ufffd\\ufffd\\ufffd\\u0015=\\u0014\\ufffd]\\ufffd\\u0005\\ufffd\\u0007\\ufffd\\ufffd!\\ufffd\\ufffd>\\u0016\\ufffd\\ufffdL)\\ufffd\\ufffd\\ufffd\\ufffdX4. < \\u001b\\ufffd\\u0003\\ufffdY\\ufffd\\ufffdu\\u024a\\u0014\\ufffd\\ufffd\\ufffdk\\ufffd\\ufffd\\u001c;j\\ufffd\\ufffd\\u0002L\\ufffd\\ufffd\\ufffdI\\\"\\ufffd~Y\\ufffd\\u0003\\ufffd\\n\\u001b\\ufffdh\\ufffd\\u000eD\\ufffdt&vx\\u0016\\ufffd_\\ufffdDw\\u0004c\\ufffd\\ufffd\\u0018\\u0426\\ufffd\\ufffde\\ufffd+\\u0244.\\ufffd'\\ufffdX0\\ufffd < \\ufffd\\ufffd\\u0000R\\ufffdmA\\ufffd\\u0012\\ufffd\\ufffdv\\ufffd\\ufffd\\ufffdi\\ufffd\\u0011\\u001f\\u6a31\\ufffda\\ufffd\\ufffdi\\ufffd]\\u0007v\\ufffdO\\ufffdc\\u0016\\ufffd\\u0002\\ufffd-\\u0013Fm\\ufffd\\ufffdIl\\u0010,\\ufffd\\n\\ufffd\\ufffd_\\ufffd\\ufffd\\ufffd\\ufffd#\\ufffdy\\ufffdl\\ufffd\\ufffdq\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffdOH\\ufffd \\ufffd(\\u0010~\\ufffd\\ufffdb\\ufffd~\\ufffd?\\u001cD\\u001e\\ufffd$O\\ufffd+R!\\ufffd\\ufffdBQym\\ufffd\\ufffd\\ufffdk\\ufffd\\u00174@\\ufffdo\\ufffd\\ufffd\\ufffd\\ufffd4\\u0014\\ufffdy\\ufffd\\ufffd\\ufffd\\ufffd\\ufffdI\\ufffd\\ufffd\\ufffdA\\ufffd\\t\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\u0010xKr9\\b\\u0015\\ufffdt\\u0002\\ufffd\\n\\ufffd\\ufffd}\\ufffd\\u0015\\ufffd\\ufffd\\ufffd...\", \"robots_allowed\": true, \"error\": null}]", "browsed": true, "browse_calls": 3, "sources": [{"url": "https://arxiv.org/abs/2109.07958", "title": "[2109.07958] TruthfulQA: Measuring How Models Mimic Human Falsehoods", "retrieved_at": "2026-02-03T08:11:12.344321+00:00", "snippets": ["The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans."], "query": "TruthfulQA: Measuring How Models Mimic Human Falsehoods paper TruthfulQA"}, {"url": "https://aclanthology.org/2022.acl-long.229/", "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods - ACL Anthology", "retrieved_at": "2026-02-03T08:11:12.344321+00:00", "snippets": ["%0 Conference Proceedings %T ... Ireland %F lin-etal-2022-truthfulqa %X We propose a benchmark to measure whether a language model is truthful in generating answers to questions...."], "query": "TruthfulQA: Measuring How Models Mimic Human Falsehoods paper TruthfulQA"}, {"url": "https://aclanthology.org/N18-1074/", "title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification - ACL Anthology", "retrieved_at": "2026-02-03T08:11:13.462846+00:00", "snippets": ["%0 Conference Proceedings %T FEVER: ... Louisiana %F thorne-etal-2018-fever %X In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERificat..."], "query": "FEVER: a Large-Scale Dataset for Fact Extraction and VERification Thorne 2018 paper FEVER dataset"}, {"url": "https://fever.ai/dataset/fever.html", "title": "Fact Extraction and VERification", "retrieved_at": "2026-02-03T08:11:13.462846+00:00", "snippets": ["@inproceedings{Thorne18Fever, author = {Thorne, James and Vlachos, Andreas and Christodoulopoulos, Christos and Mittal, Arpit}, title = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VERification}}, booktitle = {NAACL-HLT}, year = {2018} }"], "query": "FEVER: a Large-Scale Dataset for Fact Extraction and VERification Thorne 2018 paper FEVER dataset"}, {"url": "https://aclanthology.org/2022.naacl-main.187.pdf", "title": "Improved QA-Based Factual Consistency Evaluation for ...", "retrieved_at": "2026-02-03T08:11:14.492602+00:00", "snippets": ["Table 4: Example source documents and summaries along with QA-based metric component outputs. Left: This \u00b7 example illustrates that the fluency of the QG model does not necessarily improve downstream factual consistency"], "query": "QAGS QA-based Evaluation for Faithfulness Kryscinski 2020 paper (QA-based summarization factuality)"}, {"url": "https://eugeneyan.com/writing/abstractive/", "title": "Evaluation & Hallucination Detection for Abstractive Summaries", "retrieved_at": "2026-02-03T08:11:14.492602+00:00", "snippets": ["Consistency: The summary and source document should be factually consistent aka faithful (i.e., no new information in the summary that\u2019s not in the source)"], "query": "QAGS QA-based Evaluation for Faithfulness Kryscinski 2020 paper (QA-based summarization factuality)"}], "latency_ms": 49025.01833399992, "token_counts": {"prompt": null, "completion": null, "total": null}, "model_id": "gpt-5-mini"}
{"eval_id": "eval_001", "run_id": "fb74550b-3b2d-497e-9a78-b933679460b9", "started_at": "2026-02-03T08:21:06.683212+00:00", "query": "Identify 3 reputable benchmarks for evaluating LLM factuality and briefly describe what each measures. Cite original papers/sites.", "answer": "[{\"url\": \"https://arxiv.org/abs/1803.05355\", \"status\": 200, \"text\": \"[1803.05355] FEVER: a large-scale dataset for Fact Extraction and VERification Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate > cs > arXiv:1803.05355 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computation and Language arXiv:1803.05355 (cs) [Submitted on 14 Mar 2018 ( v1 ), last revised 18 Dec 2018 (this version, v3)] Title: FEVER: a large-scale dataset for Fact Extraction and VERification Authors: James Thorne , Andreas Vlachos , Christos Christodoulopoulos , Arpit Mittal View a PDF of the paper titled FEVER: a large-scale dataset for Fact Extraction and VERification, by James Thorne and 2 other authors View PDF Abstract: In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss $\\\\kappa$. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources. Comments: Updated version of NAACL2018 paper. Data is released on...\", \"robots_allowed\": true, \"error\": null}, {\"url\": \"https://arxiv.org/abs/1909.01066\", \"status\": 200, \"text\": \"[1909.01066] Language Models as Knowledge Bases? Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate > cs > arXiv:1909.01066 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computation and Language arXiv:1909.01066 (cs) [Submitted on 3 Sep 2019 ( v1 ), last revised 4 Sep 2019 (this version, v2)] Title: Language Models as Knowledge Bases? Authors: Fabio Petroni , Tim Rockt\\u00e4schel , Patrick Lewis , Anton Bakhtin , Yuxiang Wu , Alexander H. Miller , Sebastian Riedel View a PDF of the paper titled Language Models as Knowledge Bases?, by Fabio Petroni and 6 other authors View PDF Abstract: Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as \\\"fill-in-the-blank\\\" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily th...\", \"robots_allowed\": true, \"error\": null}, {\"url\": \"https://arxiv.org/abs/2109.07958\", \"status\": 200, \"text\": \"[2109.07958] TruthfulQA: Measuring How Models Mimic Human Falsehoods Skip to main content We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate > cs > arXiv:2109.07958 Help | Advanced Search All fields Title Author Abstract Comments Journal reference ACM classification MSC classification Report number arXiv identifier DOI ORCID arXiv author ID Help pages Full text Search open search GO open navigation menu quick links Login Help Pages About Computer Science > Computation and Language arXiv:2109.07958 (cs) [Submitted on 8 Sep 2021 ( v1 ), last revised 8 May 2022 (this version, v2)] Title: TruthfulQA: Measuring How Models Mimic Human Falsehoods Authors: Stephanie Lin , Jacob Hilton , Owain Evans View a PDF of the paper titled TruthfulQA: Measuring How Models Mimic Human Falsehoods, by Stephanie Lin and 2 other authors View PDF Abstract: We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web. Comments: ACL 2022 (main conferenc...\", \"robots_allowed\": true, \"error\": null}]", "browsed": true, "browse_calls": 1, "sources": [{"url": "https://aclanthology.org/N18-1074/", "title": "FEVER: a Large-scale Dataset for Fact Extraction and VERification - ACL Anthology", "retrieved_at": "2026-02-03T08:21:26.211988+00:00", "snippets": ["%0 Conference Proceedings %T FEVER: ... Louisiana %F thorne-etal-2018-fever %X In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERificat..."], "query": "FEVER \"FEVER: a Large-scale Dataset for Fact Extraction and VERification\" Thorne 2018 paper"}, {"url": "https://arxiv.org/abs/1803.05355", "title": "[1803.05355] FEVER: a large-scale dataset for Fact Extraction and VERification", "retrieved_at": "2026-02-03T08:21:26.211988+00:00", "snippets": ["View a PDF of the paper titled FEVER: a large-scale dataset for Fact Extraction and VERification, by James Thorne and 2 other authors View PDF"], "query": "FEVER \"FEVER: a Large-scale Dataset for Fact Extraction and VERification\" Thorne 2018 paper"}], "latency_ms": 42667.673238000134, "token_counts": {"prompt": null, "completion": null, "total": null}, "model_id": "gpt-5-mini"}
{"eval_id": "eval_001", "run_id": "4033dc7f-fcfb-488d-aa2c-7121bae97214", "started_at": "2026-02-03T08:42:59.771392+00:00", "query": "Identify 3 reputable benchmarks for evaluating LLM factuality and briefly describe what each measures. Cite original papers/sites.", "answer": "No fetched content was available; the descriptions below come from general knowledge. If you want direct links/papers, I can fetch them.\n\n1) FEVER \u2014 \"FEVER: a Large-Scale Dataset for Fact Extraction and VERification\" (Thorne et al., 2018)\n- What it measures: fact verification of short claims against a large knowledge source (Wikipedia). Systems must retrieve evidence sentences and classify each claim as SUPPORTED, REFUTED, or NOT ENOUGH INFO. Useful for end-to-end claim-checking and evidence retrieval evaluations.\n- Citation: Thorne et al., 2018, FEVER: a Large-Scale Dataset for Fact Extraction and VERification.\n\n2) TruthfulQA \u2014 \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\" (Lin et al., 2021)\n- What it measures: propensity of language models to produce false but socially plausible answers across a diverse set of prompts/questions. Emphasizes truthfulness (avoiding misinformation and \u201ccommon human falsehoods\u201d) rather than just task accuracy.\n- Citation: Lin et al., 2021, TruthfulQA: Measuring How Models Mimic Human Falsehoods.\n\n3) SciFact \u2014 \"SciFact: A Benchmark for Scientific Claim Verification\" (Wadden et al., 2020)\n- What it measures: verification of scientific claims against published abstracts. Systems must retrieve and present supporting or refuting evidence from scientific literature and label the claim\u2019s veracity. Designed to evaluate factuality in domain-specific (scientific) contexts.\n- Citation: Wadden et al., 2020, SciFact: A Dataset for Scientific Claim Verification.\n\nIf you\u2019d like, I can fetch the original papers/URLs and return direct links and exact bibliographic details.", "browsed": false, "browse_calls": 0, "sources": [], "latency_ms": 54106.924161999814, "token_counts": {"prompt": null, "completion": null, "total": null}, "model_id": "gpt-5-mini"}
{"eval_id": "eval_001", "run_id": "023bf4ce-446f-4357-af74-c70432ad3f4b", "started_at": "2026-02-03T08:45:41.438161+00:00", "query": "What is the latest stable release version of Python, and its release date? Cite the official source.", "answer": "According to the official Python downloads page, the latest stable release is Python 3.14.2 (source: https://www.python.org/downloads/). The fetched page text does not include the release date for 3.14.2, so I can\u2019t confirm the release date from the fetched content. \n\nIf you\u2019d like, I can fetch the specific release page (e.g. https://www.python.org/downloads/release/python-3142/) and give the exact release date with an official citation.", "browsed": true, "browse_calls": 2, "sources": [{"url": "https://devguide.python.org/versions/", "title": "Status of Python versions", "retrieved_at": "2026-02-03T08:45:49.590784+00:00", "snippets": ["The main branch is currently the future Python 3.15, and is the only branch that accepts new features. The latest release for each Python version can be found on the download page.(See below for a ..."], "query": "latest stable release of Python site:python.org"}, {"url": "https://www.python.org/downloads/", "title": "Download Python | Python.org", "retrieved_at": "2026-02-03T08:45:49.590784+00:00", "snippets": ["The same source code archive can also be used to build the Windows and Mac versions, and is the starting point for ports to all other platforms. Download the latest Python 3 source. ... This site hosts the \"traditional\" implementation of Python ..."], "query": "latest stable release of Python site:python.org"}, {"url": "https://www.python.org/downloads/", "title": "Download Python | Python.org", "retrieved_at": "2026-02-03T08:45:50.716373+00:00", "snippets": ["Python 3.14.2 Dec. 5, 2025 Download Release notes"], "query": "Python downloads latest release site:python.org/downloads"}, {"url": "https://www.python.org/downloads/windows/", "title": "Python Releases for Windows | Python.org", "retrieved_at": "2026-02-03T08:45:50.716373+00:00", "snippets": ["Downloads>>> Windows \u00b7 Latest Python install manager - Python install manager 25.2 \u00b7 Latest Python 3 Release - Python 3.14.2 \u00b7 Python 3.14.2 - Dec. 5, 2025 \u00b7 Download using the Python install manager."], "query": "Python downloads latest release site:python.org/downloads"}], "latency_ms": 26992.664006999803, "token_counts": {"prompt": null, "completion": null, "total": null}, "model_id": "gpt-5-mini"}
{"eval_id": "eval_001", "run_id": "d1d97ac6-4623-4051-a7ca-9eaa6e111972", "started_at": "2026-02-03T08:47:35.622298+00:00", "query": "How to make thai tea?", "answer": "No web sources were fetched. Below is a concise, reliable recipe for classic Thai iced tea (cha yen) and a few variation notes.\n\nIngredients (makes about 4 cups brewed tea; serves 2\u20133 iced glasses)\n- 4 cups water\n- 3\u20134 tbsp Thai tea mix (Cha Tra Mue or similar).  \n  - If you don\u2019t have Thai tea mix: use 3\u20134 tbsp strong black tea leaves (Assam or Ceylon) + 1 star anise and 1/2 tsp vanilla or a small scrap of orange peel for a closer flavor.\n- 1/3 to 1/2 cup granulated sugar (adjust to taste)\n- 1/2 cup sweetened condensed milk\n- 1/4 cup evaporated milk, half\u2011and\u2011half, or whole milk (or coconut milk for vegan)\n- Ice\n\nMethod\n1. Bring the 4 cups water to a boil. Add the Thai tea mix (or black tea + spices). Reduce heat and simmer 3\u20135 minutes for a strong brew (for an extra-robust flavor you can simmer up to 7\u201310 minutes\u2014taste to avoid over\u2011bitter).\n2. Remove from heat and strain tea through a fine mesh strainer or cheesecloth into a heatproof container.\n3. While the tea is still hot, stir in the sugar until dissolved. Let the tea cool to room temperature, then chill in the refrigerator until cold.\n4. To serve: fill a glass with ice. Pour in the chilled tea, leaving room at the top. Add about 1\u20132 tablespoons sweetened condensed milk per glass (adjust for sweetness). Top with a splash (1\u20132 tbsp) of evaporated milk/half\u2011and\u2011half/whole milk or coconut milk. The condensed milk and evaporated milk will create a creamy, layered look\u2014stir before drinking.\n\nSingle-serving guideline\n- 1 cup strong-brewed tea + 1\u20132 tbsp sweetened condensed milk + 1\u20132 tbsp evaporated milk + ice.\n\nTips/variations\n- Traditional Thai tea mix often includes spices and orange food coloring; using the mix gives the classic color and flavor.\n- For vegan: use coconut milk + sweetened condensed coconut milk (or sweeten coconut milk with sugar).\n- Make it hot: skip chilling and ice; stir in condensed and evaporated milk while hot.\n- Adjust sweetness and creaminess to taste.\n\nThat\u2019s it\u2014strongly brewed, sweetened tea plus condensed milk and a cream topper is the classic Thai tea.", "browsed": false, "browse_calls": 0, "sources": [], "latency_ms": 19247.089700000288, "token_counts": {"prompt": null, "completion": null, "total": null}, "model_id": "gpt-5-mini"}
